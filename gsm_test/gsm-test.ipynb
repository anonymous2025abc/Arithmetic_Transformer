{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"eadf4b8601b34748ad604dafd03a859e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3d0c5b0cf4747729b9d9097ba64946b","IPY_MODEL_263283a6de7b41afbf771fe871cc2b67","IPY_MODEL_206a3a11c1e244ae80ce0202a9c1ebe5"],"layout":"IPY_MODEL_b4d273463f29466e8df9f4f456d7961d"}},"a3d0c5b0cf4747729b9d9097ba64946b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a56857fda4a41a8a1685d547f799bc8","placeholder":"​","style":"IPY_MODEL_30ad43ce9af24aa99d48784bb4381e31","value":"Loading checkpoint shards: 100%"}},"263283a6de7b41afbf771fe871cc2b67":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5f563299510446d8f67d56c3ff13262","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a9ca0896dc149b58fad4cf69f6c8aad","value":2}},"206a3a11c1e244ae80ce0202a9c1ebe5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f0e9d75bbe64d4389772606b7e1ab93","placeholder":"​","style":"IPY_MODEL_273bc5aa118e4d0191b73da855a05ac6","value":" 2/2 [04:33&lt;00:00, 123.68s/it]"}},"b4d273463f29466e8df9f4f456d7961d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a56857fda4a41a8a1685d547f799bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30ad43ce9af24aa99d48784bb4381e31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5f563299510446d8f67d56c3ff13262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a9ca0896dc149b58fad4cf69f6c8aad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f0e9d75bbe64d4389772606b7e1ab93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"273bc5aa118e4d0191b73da855a05ac6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"lKiQnGGYvzAi","executionInfo":{"status":"ok","timestamp":1768838241863,"user_tz":360,"elapsed":7250,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}}},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","import transformers\n","import torch.nn.functional as F\n","import random\n","from tqdm import tqdm\n","import subprocess as sp\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","#########\n","#########\n","wd = \"drive/MyDrive/gsm_test\" # CHANGE TO YOUR WORKING DIRECTOR\n","##################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oF58mTaFv1dt","executionInfo":{"status":"ok","timestamp":1768838306225,"user_tz":360,"elapsed":558,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"86b112c4-9401-4448-f55c-0f62c4d36314"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["##########\n","file = open(\"drive/My Drive/gsm_test/HF_token.txt\", \"r\") # REQUIRE HUGGINGFACE TOKEN TO ACCESS CERTAIN MODELS, SETUP TOKEN, SAVE TO TEXT\n","##########\n","\n","HF_token = file.readline()\n","file.close()\n","os.environ[\"HF_TOKEN\"] = HF_token\n","\n","#!pip install -U \"huggingface_hub[cli]\"\n","!huggingface-cli login --token $HF_TOKEN --add-to-git-credential\n","!git config --global credential.helper store"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YqfjSY75wDDQ","executionInfo":{"status":"ok","timestamp":1768838310379,"user_tz":360,"elapsed":2651,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"3e8e092f-2994-42df-88e4-890f3c77712a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n","Token is valid (permission: read).\n","The token `MyFirstToken` has been saved to /root/.cache/huggingface/stored_tokens\n","\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub.\n","Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n","\n","git config --global credential.helper store\n","\n","Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","Token has not been saved to git credential helper.\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"]}]},{"cell_type":"code","source":["######### Specify a folder at Google drive to save downloaded pretrained models\n","###########\n","model_path = \"drive/My Drive/Colab Notebooks/huggingface_models\"\n","###########\n","\n","def create_folder(dir):\n","    if not os.path.isdir(dir):\n","        os.mkdir(dir)\n","\n","def get_gpu_memory():\n","    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n","    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n","    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n","    return memory_free_values\n","\n","def fix_random_seed(seed, reproduce=False):\n","    # torch.backends.cudnn.enabled = True\n","    # torch.backends.cudnn.benchmark = True\n","\n","    if reproduce:\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        ## NOTE: uncomment for CUDA >= 10.2\n","        # os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n","        ## NOTE: uncomment for pytorch >= 1.8\n","        # torch.use_deterministic_algorithms(True)\n","\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    rng = torch.manual_seed(seed)\n","\n","    return rng"],"metadata":{"id":"U2QZ9EjXwUzh","executionInfo":{"status":"ok","timestamp":1768838317234,"user_tz":360,"elapsed":50,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["model_dict = {\n","    1: \"mistral-7b\",  # Checked\n","    2: \"llama2-7b\",\n","    3: \"llama3-8b\",\n","    4: \"llama3-8B-Instruct\",  # Checked\n","    5: \"gemma2-9b-it\",\n","    6: \"qwen-1.5B\",   # Checked\n","    7: \"qwen-3B\",  # Checked\n","    8: \"qwen-7B\",  # Checked\n","    ## The following are math reasoning models\n","    9: \"llama3.1-8b\",\n","    10: \"qwen2.5-7b\",  # Checked, all correct\n","    11: \"mathstral-7b\",  # Checked, incorrect on questions with most operands\n","    12: \"deepseek-7b\",  # Checked, incorrect on last two questions with most operands\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zWMJqVzawZ3W","executionInfo":{"status":"ok","timestamp":1768838319580,"user_tz":360,"elapsed":17,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"0af66b5f-105d-45b3-ee3a-ec16132efc85"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["pythia 7b\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM\n","from transformers import AutoTokenizer\n","import random\n","\n","random.seed(2026)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch_dtype = torch.float16\n","output_hidden_states = False\n","output_attentions = False\n","skip = False # skip special tokens\n","trust_remote_code = True if model_name == \"qwen\" else False\n","\n","if model_choice == \"gpt2-medium\":\n","  llm_name = 'gpt2-medium'\n","elif model_choice == \"mistral-7b\":\n","  llm_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","elif model_choice == \"gemma2-9b\":\n","  llm_name = \"google/gemma-2-9b\"\n","elif model_choice == \"llama2-7b\":\n","  llm_name = \"meta-llama/Llama-2-7b-chat-hf\"\n","elif model_choice == \"llama3-8b\":\n","  llm_name = \"meta-llama/Meta-Llama-3-8B\"\n","elif model_choice == \"gptJ-6b\":\n","  llm_name = \"EleutherAI/gpt-j-6B\"\n","elif model_choice == \"llama3-8B-Instruct\":\n","  llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n","  torch_dtype = torch.bfloat16\n","elif model_choice == \"gemma2-9b-it\":\n","  llm_name = \"google/gemma-2-9b-it\"\n","  torch_dtype = torch.bfloat16\n","elif model_choice == \"qwen-1.5B\":\n","  llm_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","elif model_choice == \"qwen-3B\":\n","  llm_name = \"Qwen/Qwen2.5-3B-Instruct\"\n","elif model_choice == \"qwen-7B\":\n","  llm_name = \"Qwen/Qwen2.5-7B-Instruct\"\n","elif model_choice == \"llama3.1-8b\":\n","  llm_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","elif model_choice == \"qwen2.5-7b\":\n","  llm_name = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n","elif model_choice == \"mathstral-7b\":\n","  llm_name = \"mistralai/Mathstral-7B-v0.1\"\n","elif model_choice == \"deepseek-7b\":\n","  llm_name = \"deepseek-ai/deepseek-math-7b-instruct\"\n","\n","\n","# now loading model\n","if model_choice == \"pythia-7b\":\n","  from transformers import GPTNeoXForCausalLM\n","  model = GPTNeoXForCausalLM.from_pretrained(\n","  \"EleutherAI/pythia-6.9b-deduped\",\n","  revision=\"step3000\",\n","  cache_dir=model_path,\n","  output_hidden_states=output_hidden_states\n",")\n","\n","  tokenizer = AutoTokenizer.from_pretrained(\n","    \"EleutherAI/pythia-6.9b-deduped\",\n","    revision=\"step3000\",\n","    cache_dir=model_path,\n","  )\n","else:\n","  model = AutoModelForCausalLM.from_pretrained(llm_name, cache_dir=model_path, trust_remote_code=trust_remote_code,\n","                                               torch_dtype=\"auto\", device_map=\"auto\")\n","  tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side='left', trust_remote_code=trust_remote_code)\n","\n","\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","model.generation_config.pad_token_id = tokenizer.pad_token_id\n","\n","model.eval()\n","vocab_size = model.config.vocab_size\n","n_layer = model.config.num_hidden_layers\n","H = model.config.hidden_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176,"referenced_widgets":["eadf4b8601b34748ad604dafd03a859e","a3d0c5b0cf4747729b9d9097ba64946b","263283a6de7b41afbf771fe871cc2b67","206a3a11c1e244ae80ce0202a9c1ebe5","b4d273463f29466e8df9f4f456d7961d","5a56857fda4a41a8a1685d547f799bc8","30ad43ce9af24aa99d48784bb4381e31","f5f563299510446d8f67d56c3ff13262","7a9ca0896dc149b58fad4cf69f6c8aad","1f0e9d75bbe64d4389772606b7e1ab93","273bc5aa118e4d0191b73da855a05ac6"]},"id":"hGPMVfeawf_Z","executionInfo":{"status":"ok","timestamp":1768838627086,"user_tz":360,"elapsed":305445,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"e65ddf3a-a514-47bc-cc85-c2d5796b3c1b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eadf4b8601b34748ad604dafd03a859e"}},"metadata":{}}]},{"cell_type":"code","source":["def gen_clauses(k=0):\n","  names = [\"Seattle\", \"Alice\", \"Bob\", \"David\", \"Zoe\", \"John\", \"Rachel\"]\n","  multipliers = [\"twice\", \"5 times\", \"3 times\", \"twice\", \"4 times\", \"5 times\"]\n","  factors = [2, 5, 3, 2, 4, 5]\n","  assert k < 7\n","  clauses = \"\"\n","  q_names = \"and Seattle\" if k==0 else \", \".join(names[:k]) + \" and \" + names[k]\n","  last_name = names[k]\n","  correct = ((2+1)*4)+1\n","  for j in range(k):\n","    clauses += f\"{names[j]} has {multipliers[j]} as many sheep as {names[j+1]}. \"\n","    correct = correct * factors[j] + 1\n","  correct *= 20\n","  return clauses, q_names, last_name, correct\n","\n","clauses, q_names, last_name, correct = gen_clauses(3)\n","question = f\"Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. {clauses}How many sheep do Toulouse, Charleston, {q_names} have together if {last_name} has 20 sheep?\"\n","print(question)\n","print(correct)\n","print('='*50)\n","\n","def gen_clauses_2(k=0):\n","  num_customers = [4, 2, 8, 3, 7, 5]\n","  num_DVDs = [3, 8, 9, 2, 1, 9]\n","  assert k < 7\n","  clauses = \"\"\n","  correct = 3 + 2*2\n","  for j in range(k):\n","    item = \"DVDs\" if num_DVDs[j] > 1 else \"DVD\"\n","    clauses += f\"His next {num_customers[j]} customers buy {num_DVDs[j]} {item} each. \"\n","    correct += num_customers[j] * num_DVDs[j]\n","  return clauses, correct\n","\n","clauses, correct = gen_clauses_2(3)\n","question2 = f\"Billy sells DVDs. He has 8 customers on Tuesday. His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. {clauses}His last 3 customers don't buy any DVDs. How many DVDs did Billy sell on Tuesday?\"\n","print(question2)\n","print(correct)\n","print('='*50)\n","\n","def gen_clauses_3(k=0):\n","  num_pairs = [2, 3, 1, 3, 4, 5]\n","  items = [\"suspenders\", \"socks\", \"sunglasses\", \"gloves\", \"earrings\", \"slippers\"]\n","  prices = [20.50, 5.00, 30.00, 21.50, 94.50, 20.50]\n","  assert k < 7\n","  clauses = \"\"\n","  clauses2 = \"\"\n","  correct = 3*16.5 + 3*22.5 + 3*42\n","  for j in range(k):\n","    clauses += f\"{num_pairs[j]} pairs of {items[j]}, \"\n","    clauses2 += f\"One pair of {items[j]} costs ${prices[j]}. \"\n","    correct += num_pairs[j] * prices[j]\n","  return clauses, clauses2, correct\n","\n","clauses, clauses2, correct = gen_clauses_3(3)\n","question3 = f\"Mishka bought 3 pairs of shorts, {clauses}3 pairs of pants, and 3 pairs of shoes. One pair of shorts costs $16.50. {clauses2}One pair of pants costs $22.50 and one pair of shoes costs $42. How many dollars did Mishka spend on all the clothing items?\"\n","print(question3)\n","print(correct)\n","print('='*50)\n","\n","def gen_questions(k=0, id=0):\n","  if id == 0:\n","    clauses, q_names, last_name, correct = gen_clauses(k)\n","    question = f\"Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. {clauses}How many sheep do Toulouse, Charleston, {q_names} have together if {last_name} has 20 sheep?\"\n","  elif id == 1:\n","    clauses, correct = gen_clauses_2(k)\n","    question = f\"Billy sells DVDs. He has 8 customers on Tuesday. His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. {clauses}His last 3 customers don't buy any DVDs. How many DVDs did Billy sell on Tuesday?\"\n","  elif id == 2:\n","    clauses, clauses2, correct = gen_clauses_3(k)\n","    question = f\"Mishka bought 3 pairs of shorts, {clauses}3 pairs of pants, and 3 pairs of shoes. One pair of shorts costs $16.50. {clauses2}One pair of pants costs $22.50 and one pair of shoes costs $42. How many dollars did Mishka spend on all the clothing items?\"\n","  return question, correct"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGJp3eHAxyCA","executionInfo":{"status":"ok","timestamp":1768838630154,"user_tz":360,"elapsed":34,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"e4729464-a6f8-4ea9-f2cd-e454884e3d8b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. Seattle has twice as many sheep as Alice. Alice has 5 times as many sheep as Bob. Bob has 3 times as many sheep as David. How many sheep do Toulouse, Charleston, Seattle, Alice, Bob and David have together if David has 20 sheep?\n","8180\n","==================================================\n","Billy sells DVDs. He has 8 customers on Tuesday. His first 3 customers buy one DVD each. His next 2 customers buy 2 DVDs each. His next 4 customers buy 3 DVDs each. His next 2 customers buy 8 DVDs each. His next 8 customers buy 9 DVDs each. His last 3 customers don't buy any DVDs. How many DVDs did Billy sell on Tuesday?\n","107\n","==================================================\n","Mishka bought 3 pairs of shorts, 2 pairs of suspenders, 3 pairs of socks, 1 pairs of sunglasses, 3 pairs of pants, and 3 pairs of shoes. One pair of shorts costs $16.50. One pair of suspenders costs $20.5. One pair of socks costs $5.0. One pair of sunglasses costs $30.0. One pair of pants costs $22.50 and one pair of shoes costs $42. How many dollars did Mishka spend on all the clothing items?\n","329.0\n","==================================================\n"]}]},{"cell_type":"markdown","source":["### Zero-shot prompting"],"metadata":{"id":"H7P0xKkOHDHl"}},{"cell_type":"code","source":["\"\"\"\n","messages = [\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Solve the math problem. End with: #### <final answer>\"},\n","    {\"role\": \"user\", \"content\": question},\n","]\n","\n","prompt = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True,  # adds the assistant header for you\n",")\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","# Important: Llama-3 chat templates use <|eot_id|>; include it as a stopping token.\n","# Some HF configs don’t stop on eot_id unless you pass it explicitly. :contentReference[oaicite:2]{index=2}\n","eos_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n","\n","out = model.generate(\n","    **inputs,\n","    max_new_tokens=256,\n","    eos_token_id=eos_ids,\n","    do_sample=False,\n",")\n","\n","print(tokenizer.decode(out[0], skip_special_tokens=True))\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"G0jV2gJuyTEq","executionInfo":{"status":"ok","timestamp":1767604265140,"user_tz":360,"elapsed":14,"user":{"displayName":"Joe Yiqiao Zhong (Joe)","userId":"04476449371835859088"}},"outputId":"880a95d6-1fe5-4e63-e751-09d696ab4124"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Solve the math problem. End with: #### <final answer>\"},\\n    {\"role\": \"user\", \"content\": question},\\n]\\n\\nprompt = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,  # adds the assistant header for you\\n)\\n\\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\\n\\n# Important: Llama-3 chat templates use <|eot_id|>; include it as a stopping token.\\n# Some HF configs don’t stop on eot_id unless you pass it explicitly. :contentReference[oaicite:2]{index=2}\\neos_ids = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\\n\\nout = model.generate(\\n    **inputs,\\n    max_new_tokens=256,\\n    eos_token_id=eos_ids,\\n    do_sample=False,\\n)\\n\\nprint(tokenizer.decode(out[0], skip_special_tokens=True))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["questions = []\n","ground_truths = []\n","for id in range(3):\n","  for k in range(6):\n","    question, correct = gen_questions(k, id)\n","    questions.append(question)\n","    ground_truths.append(correct)\n","\n","# --- helper: safe prompt builder ---\n","def safe_build_prompt(tokenizer, system_text: str, user_text: str, model_name=None):\n","    \"\"\"\n","    Returns a tokenizable prompt string (or chat template) that works with or without\n","    tokenizer.chat_template being set.\n","    \"\"\"\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_text},\n","        {\"role\": \"user\",   \"content\": user_text},\n","    ]\n","\n","    # If tokenizer supports apply_chat_template and has a chat_template configured, use it.\n","    try:\n","        if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None:\n","            # tokenizer.apply_chat_template returns a string\n","            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        # some tokenizers expose apply_chat_template but require a template arg\n","        if hasattr(tokenizer, \"apply_chat_template\"):\n","            # attempt with explicit template if that exists\n","            try:\n","                return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","            except Exception:\n","                pass\n","    except Exception:\n","        # if anything goes wrong, fallback to manual\n","        pass\n","\n","    # Fallback: plain text chat-style prompt. This is generic and usually works.\n","    # Adjust separator if your model expects special tokens (e.g., \"<|assistant|>\" etc.)\n","    prompt = f\"SYSTEM: {system_text}\\n\\nUSER: {user_text}\\n\\nASSISTANT:\"\n","    return prompt\n","\n","# --- Use it in your code ---\n","def build_prompt(q: str) -> str:\n","    system_text = \"Solve the problem. Give the final answer as: #### <number>\"\n","    # make special-case for qwen if you want a different instruction\n","    if model_name == \"qwen\":\n","        system_text = \"You are a helpful assistant that solves math word problems.\"\n","        user_text = (\n","            \"Solve the problem step by step. Show your reasoning.\\n\"\n","            \"At the end, give the final answer on its own line in the format:\\n\"\n","            \"#### <number>\\n\\n\"\n","            f\"{q}\"\n","        )\n","    else:\n","        user_text = q\n","\n","    return safe_build_prompt(tokenizer, system_text, user_text, model_name=model_name)\n","\n","prompts = [build_prompt(q) for q in questions]\n","\n","enc = tokenizer(\n","    prompts,\n","    return_tensors=\"pt\",\n","    padding=True,       # <- handles different lengths\n",").to(model.device)\n","\n","print(enc['input_ids'].shape)\n","eos_ids = [x for x in [\n","    tokenizer.eos_token_id,\n","    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n","] if x is not None]\n","\n","max_new_tokens = 1024 if model_name == \"qwen\" else 512\n","\n","out = model.generate(\n","    **enc,\n","    max_new_tokens=max_new_tokens,\n","    do_sample=False,\n","    eos_token_id=eos_ids if len(eos_ids) > 1 else eos_ids[0],\n",")\n","\n","decoded = tokenizer.batch_decode(out, skip_special_tokens=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3erDP4I9E1-","executionInfo":{"status":"ok","timestamp":1768840396653,"user_tz":360,"elapsed":1703252,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"a9bc60dc-c47c-4e5d-b505-063d3c13ac7f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([18, 171])\n"]}]},{"cell_type":"code","source":["import re\n","\n","def extract_number(text: str):\n","    pattern = re.compile(r\"(?:####|answer\\s+is)\\s*(.*)\", re.IGNORECASE)\n","    matches = pattern.findall(text)\n","    return matches[-1].strip() if matches else None\n","\n","def extract_last_number(text: str):\n","    nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n","    return nums[-1] if nums else None\n","\n","for i, generated in enumerate(decoded):\n","  #print(i, ground_truths[i], \"|||||\", generated[-15:])\n","  #print(i, ground_truths[i], \"|||||\", extract_number(generated))\n","  print(i, ground_truths[i], \"|||||\", extract_last_number(generated))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8krhscvCK9wq","executionInfo":{"status":"ok","timestamp":1768844385428,"user_tz":360,"elapsed":16,"user":{"displayName":"Yiqiao Zhong","userId":"06729372927322080600"}},"outputId":"b0a01ec2-afe7-440b-a615-ca6d3c37f2ec"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["0 260 ||||| 4\n","1 540 ||||| 20\n","2 2720 ||||| 4\n","3 8180 ||||| 3\n","4 16380 ||||| 20\n","5 65540 ||||| 20\n","6 7 ||||| 3\n","7 19 ||||| 3\n","8 35 ||||| 4\n","9 107 ||||| 4\n","10 113 ||||| 4\n","11 120 ||||| 2\n","12 243.0 ||||| 1.00\n","13 284.0 ||||| 1.00\n","14 299.0 ||||| 1.00\n","15 329.0 ||||| 1.00\n","16 393.5 ||||| 20.00\n","17 771.5 ||||| 000\n"]}]},{"cell_type":"code","source":["decoded[4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"Q7UKOYlCO4Bm","executionInfo":{"status":"ok","timestamp":1767604327470,"user_tz":360,"elapsed":8,"user":{"displayName":"Joe Yiqiao Zhong (Joe)","userId":"04476449371835859088"}},"outputId":"1b5ad66e-d261-4326-c9ef-881bb520176a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Solve the problem. Give the final answer as: #### <number>\\n\\nUser: Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. Seattle has twice as many sheep as Alice. Alice has 5 times as many sheep as Bob. Bob has 3 times as many sheep as David. David has twice as many sheep as Zoe. How many sheep do Toulouse, Charleston, Seattle, Alice, Bob, David and Zoe have together if Zoe has 20 sheep?\\n\\nAssistant: If Zoe has 20 sheep, then David has 2 * 20 = 40 sheep.\\nIf David has 40 sheep, then Bob has 40 / 3 = 13.33 sheep.\\nSince we can't have a fraction of a sheep, we'll round down to 13 sheep for Bob.\\nIf Bob has 13 sheep, then Alice has 5 * 13 = 65 sheep.\\nIf Alice has 65 sheep, then Seattle has 65 / 2 = 32.5 sheep.\\nAgain, we'll round down to 32 sheep for Seattle.\\nIf Seattle has 32 sheep, then Charleston has 4 * 32 = 128 sheep.\\nIf Charleston has 128 sheep, then Toulouse has 2 * 128 = 256 sheep.\\nIn total, Toulouse, Charleston, Seattle, Alice, Bob, David, and Zoe have 256 + 128 + 32 + 65 + 13 + 40 + 20 = 558 sheep together.\\nThe answer is $\\\\boxed{558}$.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}]}]}